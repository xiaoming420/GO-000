学习笔记
---
# 微服务可用性设计
> 《The site Reliability Workbook 2 》  
> 《SRE Google 运维解密》《Google SRE 工作手册》

> 《代码整洁之道》

## 隔离
隔离，本质上是对系统或资源进行分割，从而实现当系统发生故障时能限定传播范围和影响范围，
即发生故障后只有出问题的服务不可用，保证其他服务仍然可用。

### 服务隔离  
#### 动静分离
小到 CPU 的 cache line **[false sharing](https://www.cnblogs.com/cyfonly/p/5800758.html)** 、
数据库 MySQL 表设计中避免 buffer pool 频繁过期，隔离动静表，
大到架构设计中的图片、静态资源等缓存加速。本质上都体现的一样的思路，
即加速/缓存访问变换频次小的内容。  
比如 CDN 场景中，将静态资源和动态API分离，也是体现了隔离的思路：
* 降低应用服务器负载，静态文件访问负载全部通过CDN。
* 对象存储存储费用最低。
* 海量存储空间，无需考虑存储架构升级。
* 静态CDN带宽加速，延迟低。

![dynamic-and-static-separation-architecture.png](dynamic-and-static-separation-architecture.png)

> 利用 （动态）CDN 的边缘计算能力，在数据收集/上报场景中，在 agent 上先行汇总数据，
> 再批量上报到机房 ECS 服务(扇入)，可以减少大量请求，减少大量网络建立断开的开销，
> 减少 ECS 节点，节省开支。边缘计算成本比较低。

#### 读写分离
主从、Replicaset、CQRS

![archive-db-rw-separation.png](archive-db-rw-separation.png)

archive: 稿件表，存储稿件的名称、作者、分类、tag、状态等信息，表示稿件的基本信息。  
在一个投稿流程中，一旦稿件创建改动的频率比较低。

archive_stat: 稿件统计表，表示稿件的播放、点赞、收藏、投币数量，比较高频的更新。  
随着稿件获取流量，稿件被用户所消费，各类计数信息更新比较频繁。

MySQL BufferPool 是用于缓存 DataPage 的，DataPage 可以理解为缓存了表的行，
那么如果频繁更新 DataPage 不断会置换，会导致命中率下降的问题，
所以我们在表设计中，仍然可以沿用类似的思路，
其主表基本更新，在上游 Cache 未命中，透穿到 MySQL，仍然有 BufferPool 的缓存。

### 轻重隔离
#### 核心隔离
业务按照 Level 进行资源池划分（L0/L1/L2）。
* 核心/非核心的故障域的差异隔离（机器资源、依赖资源）。
* 多集群，通过冗余资源来提升吞吐和容灾能力。

![level-separation.png](level-separation.png)

#### 快慢隔离

我们可以把服务的吞吐想象为一个池，当突然洪流进来时，池子需要一定时间才能排放完，
这时候其他支流在池子里待的时间取决于前面的排放能力，耗时就会增高，对小请求产生影响。

> 扩展：Kafka 设计上的缺陷：如果 partition 数过多，因为每个 partition 会生成一个日志文件，
> 太多的文件进行顺序写在全局上来看就退化为磁盘的随机写了，IO性能会急剧下降。
> 
> 如果设计上不得不将一个 topic 拆分成很多 partition ，可以考虑使用多套集群物理拆分，
> 在发布端进行 hash 将不同的 key 发布到不同的集群上。
> 尽量不要在同一套群集中拆分过多 partition。

日志传输体系的架构设计中，整个流都会投放到一个 kafka topic 中(早期设计目的: 更好的顺序IO)，
流内会区分不同的 logID，logID 会有不同的 sink 端，它们之前会出现差速，
比如 HDFS 抖动吞吐下降，ES 正常水位，全局数据就会整体反压。

按照各种纬度隔离：sink、部门、业务、logID、重要性(S/A/B/C)。

业务日志也属于某个 logID，日志等级就可以作为隔离通道。

![logging-transfer.png](logging-transfer.png)

#### 热点隔离

热点即经常访问的数据。
很多时候我们希望统计某个热点数据访问频次最高的 Top K 数据， 并对其访问进行缓存。  
比如：
* 小表广播：从 remote cache 提升为 local cache，app 定时更新，
  甚至可以让运营平台支持广播刷新 local cache。 (使用 atomic.Value)
  ![period-poll.png](period-poll.png)
* 主动预热：比如直播房间页高在线情况下 bypass 监控主动防御。
  ![active-preheating.png](active-preheating.png)

### 物理隔离  
#### 线程隔离（非Go语言）
> 对于 Go 来说，所有 IO 都是 Nonblocking，且托管给了 Runtime，
> 只会阻塞Goroutine，不阻塞 M（即线程），我们只需要考虑 Goroutine 总量的控制，
> 不需要线程模型语言的线程隔离。

主要通过线程池进行隔离，也是实现服务隔离的基础。
把业务进行分类并交给不同的线程池进行处理，当某个线程池处理一种业务请求发生问题时，
不会讲故障扩散和影响到其他线程池，保证服务可用。

![tomcat-web-app-1.png](tomcat-web-app-1.png)
![tomcat-web-app-2.png](tomcat-web-app-2.png)
![tomcat-web-app-3.png](tomcat-web-app-3.png)
> 局部失败，Fail fast，进行降级兼容

> Java 除了线程池隔离，也有基于信号量的做法。
> 
> 传统基于线程池的做法：
> ![thread-pool.png](thread-pool.png)
> 当线程池达到 maxSize 后，再请求会触发 fallback 接口进行熔断。
> 基于信号量：
> ![semaphore.png](semaphore.png)
> 当信号量达到 maxConcurrentRequests 后，再请求会触发 fallback。
> 
> 灵魂拷问：信号量或线程池大小基于什么指标设定？Magic Number...

#### 进程隔离
主要是基于容器化（docker）和容器编排引擎（k8s）。

B站 15 年在 KVM 上部署服务；
16年使用 Docker Swarm；
16年下半年开始迁移到 Kubernetes，到17年底在线应用就全托管了，
之后很快在线应用弹性公有云上线；
20年离线 Yarn 和 在线 K8s 做了在离线混部(错峰使用)，
之后计划弹性公有云配合自建 IDC 做到离线的混合云架构。

#### 集群隔离 & 机房隔离
回顾 gRPC，我们介绍过多集群方案，即逻辑上是一个应用，物理上部署多套应用，通过 cluster 区分。

![account-service-multiple-instances.png](account-service-multiple-instances.png)
多活建设完毕后，我们应用可以划分为： region.zone.cluster.appid

### 案例
#### Case 1
早期转码集群被超大视频攻击，导致转码大量延迟。

用户上传的高清视频需要被转码成不同分辨率的视频供不同网络、不同设备的用户观看。

攻击者使用不同的账号上传大量重复内容组成的超大体积视频，将所有转码服务节点占用，
正常用户上传的普通大小视频迟迟得不到转码，大量任务被积压。

处理方式：应用轻重隔离的思想，将转码集群划分为多套集群，
分别处理大中小不同体积的视频内容，即使大体积视频转码任务积压也不会影响中小视频转码任务。

> 服务部分有损，全局可用。

#### Case 2
入口 Nginx(SLB) 故障，影响全机房流量。

问题，下游内部服务延迟，请求大量积压在 SLB 且 SLB没有灾备，最终 SLB Down 掉，服务全局不可用。

全局入口设备、基础设施要有灾备。可以按业务划分不同入口。

#### Case 3
缩略图服务，被大图实时缩略吃完所有 CPU，导致正常的小图缩略被丢弃，大量 503。

Gif 格式文件任务吃掉了其他格式的资源，
与案例1解决方法类似，为 Gif 格式文件隔离出独立的服务集群，并使用弹性扩缩容。

#### Case 4
数据库实例 cgroup 未隔离，导致大 SQL 引起的集体故障。

使用物理机部署 MySQL （基于性能考虑），但在同一台物理机上部署了多个 MySQL Daemon。
其中某个实例因为大 SQL 导致占用了物理机的所有 CPU、IO，导致其他实例性能抖动或不可用。

为各个实例加 Cgroup 限制。

> 不使用多个物理机分别单独部署 MySQL Daemon 是基于成本的考虑，
> 机房托管，占用的机位越少成本越低。

#### Case 5
INFO 日志里过大，导致异常 ERROR 日志采集延迟。

按日志级别，将日志分散到不同的 topic 采集。

## 超时控制
### 概述
Fail Fast，没有什么比挂起的请求和无响应的界面更令人失望。  
无意义的等待超时不仅浪费资源，而且还会证用户体验变得更差。  
我们的服务是互相调用的，所以在这些延迟叠加前，应该特别注意防止那些超时的操作。
* 网路传递具有不确定性。  
  连接超时、写超时、读超时，一个都别错配，少配一个都可能“炸”
  ![network-timeout.png](network-timeout.png)
* 客户端和服务端不一致的超时策略导致资源浪费。  
  客户端1s超时，服务端2s超时，请求后1s，客户端认为超时，报错返回，
  但服务端还会再执行1s才发现超时，形成浪费
* “默认值”策略  
  Kit 库要提供一个合理的默认值，最好对明显不合理的超时配置有一定检测能力，防止错配。
* 高延迟服务导致 client 浪费资源等待，使用超时传递：进程内传递 + 跨进程传递。

> 超时控制是微服务可用性的第一道关，良好的超时策略，可以尽可能让服务不堆积请求，
> 尽快清空高延迟的请求，释放 Goroutine。

> 只有服务不挂，才有机会执行各种降级、容错、熔断策略，如果服务挂了，什么都白搭。

### 超时契约
实际业务开发中，依赖的微服务的超时策略并不清楚，或者随着业务迭代耗时发生了变化，
意外的导致依赖者出现了超时报错。

如服务 A 依赖服务 B，B 约定了95%响应一定可以在 200ms 内返回 ，
A 按照 B 的保证配置了 200ms 的调用超时，
但随着 B 的迭代，响应时间越来越长，A 服务开始大量报错。

> * SLI 服务质量/水平指标
> * SLO 服务质量/水平目标
> * SLA 服务质量/水平协议/保证

君子协定：
服务提供都定义好 latency SLO，更新到 gRPC 的 proto 定义中，
服务的后续迭代都应保证SLO。
![latency-slo.png](latency-slo.png)

### 避免意外配置
避免出现意外的默认超时策略，或者意外的配置超时策略。
* kit 基础库兜底默认超时，比如 100ms，进行配置防御保护，避免出现类似 60s 之类的超大超时策略。
* 配置中心公共模版，对于未配置的服务使用公共配置。

### 超时传递
当上游服务已经超时返回504，但下游服务仍然在执行，会导致浪费资源做无用功。  
超时传递指的是把当前服务的剩余 Quota 传递到下游服务中，接力超时策略，控制请求级别的全局超时控制。

#### 进程内超时传递
一个请求在每个阶段（网络请求）开始前，就要检查是否还有足够的剩余来处理请求，
以及继承他的超时策略。

使用 Go 标准库的 context.WithTimeout。
```
func (c *asiiConn) Get(ctx context.Context, key string) (result *Item, err error) {
	c.conn.SetWriteDeadline(shrinkDeadline(ctx, c.writeTimeout))
	if _, err = fmt.Fprintf(c.rw, "gets %s\r\n", key); err != nil {
...
```
[shrinkDeadline](https://github.com/go-kratos/kratos/blob/master/pkg/cache/redis/util.go#L8)

#### 服务间超时传递
gRPC 不仅支持超时传递还支持级联取消。

在 gRPC 框架中，会依赖 gRPC Metadata Exchange，
基于 HTTP2 的 Headers 传递 grpc-timeout 字段，
自动传递到下游，构建带 timeout 的 context。

在需要强制执行时，下游的服务可以覆盖上游的超时传递和配额。

![grpc-timeout-propagate.png](grpc-timeout-propagate.png)

### 经验
* 双峰分布: 95%的请求耗时在100ms内，5%的请求可能永远不会完成(长超时)。
* 对于监控不要只看mean，可以看看耗时分布统计，比如 95th，99th。
* 设置合理的超时，拒绝超长请求，或者当Server 不可用要主动失败。

> 超时意味着服务线程耗尽，对于Go语言来说会导致 Goroutine 堆积，引发 OOM。

### 案例
#### Case 1
SLB 入口 Nginx 没配置超时导致连锁故障。

没有配置 proxy_timeout。

现在固定配置 1s 超时 504

#### Case 2
服务依赖的 DB 连接池漏配超时，导致请求阻塞，最终服务集体 OOM。

很多连接池实现，在调用者请求连接池获取连接时，默认超时等待为0，即不超时。
当连接池中没有可用连接、且连接数已达最大时，大量 Goroutine 阻塞在获取连接方法，最终 OOM。

获取连接时使用一个较短的超时时间如100ms或没有可用连接时直接报错。 Fail Fast。  
在选用连接池时，优先选择接收 context 、接受 context 超时控制的实现。

#### Case 3
下游服务发版耗时增加，而上游服务配置超时过短，导致上游请求失败。

广告服务，需要调用广告库存，广告库存需要请求很多外围广商的接口进行广告竞价，响应时间不可控。

没什么太好的办法，只能商务上要求外围广商保证接口 SLO 。

## 过载保护
### 令牌桶算法
是一个存放固定容量令牌的桶，按照固定速率向桶中添加令牌。停牌桶算法的描述如下：

1. 假设限制 2r/s，则按照 500 毫秒的固定速率向桶中添加令牌。
2. 桶中最多存放 b 个令牌，当桶满时，新添加的令牌被丢弃或拒绝。
3. 当一个 n 个字节大小的数据包到达，将从桶中删除 n 个令牌，接着数据包被发送到网络上。
4. 如果桶中的令牌不足 n 个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。

![token-bucket.png](token-bucket.png)

> token-bucket rate limit algorithm [/x/time/rate](https://pkg.go.dev/golang.org/x/time/rate)

令牌桶算法，填充的速率是固定的，但允许请求速率暴发一下 burst。

### 漏桶算法
作为计量工具时，可以用于流量整型和流量控制。漏桶算法的描述如下：

1. 一个固定容量的漏桶，按照常量固定速率流出水滴。
2. 如果桶是空的，则不需流出水滴。
3. 可以以任意速率注入水滴到漏桶。
4. 如果注入水滴超出了桶的容量，则流入的水滴溢出（被丢弃），而漏桶容量是不变的。

![leaky-bucket.png](leaky-bucket.png)

> leaky-bucket rate limit algorithm: [/go.uber.org/ratelimit](https://pkg.go.dev/go.uber.org/ratelimit)

### 令牌桶/漏桶问题
令牌桶/漏桶确实能够保护系统不被拖垮,
但不管漏桶还是令牌桶, 其防护思路都是设定一个指标,
当超过该指标后就阻止或减少流量的继续进入，
当系统负载降低到某一水平后则恢复流量的进入。
但其通常都是被动的，其实际效果取决于限流阈值设置是否合理，
但往往设置合理不是一件容易的事情。

* 集群增加机器或者减少机器限流阈值是否要重新设置?
* 设置限流阈值的依据是什么?
* 人力运维成本是否过高?
* 当调用方反馈429时, 这个时候重新设置限流, 其实流量高峰已经过了重新评估限流是否有意义?

这些其实都是采用漏桶/令牌桶的缺点, 总体来说就是太被动, 不能快速适应流量变化。
因此我们需要一种自适应的限流算法，即: 过载保护，根据系统当前的负载自动丢弃流量。

> 扩展：应用上线前要做极限压测。
> 几乎所有的压测系统都会在系统报错率达到一定水平后自动停压。
> 但生产线上真正出现过载事故时，来自用户的请求压力不会停止，
> 反而会在系统无法承受，服务质量下降时加倍的反复请求，以期得到满意的服务。
> 不做极限压测，无法得知在这种压力条件下系统的表现会如何。

### 过载保护
计算系统临近过载时的峰值吞吐作为限流的阈值来进行流量控制，达到系统保护。
* 服务器临近过载时，主动抛弃一定量的负载，目标是自保。
* 在系统稳定的前提下，保持系统的吞吐量。

常见做法：**利特尔法则**

![little's-law.png](little's-law.png)

> 系统容量/系统吞吐 = QPS * Latency

* CPU、内存作为信号量进行节流。
* 队列管理: 队列长度、LIFO。
* 可控延迟算法: [CoDel](https://blog.csdn.net/dog250/article/details/72849893) + BBR。

如何计算接近峰值时的系统吞吐？
* CPU：使用一个独立的线程（协程）采样，每隔 250ms 触发一次。  
  在计算均值时，使用简单滑动平均或加权滑动平均，去除峰值的影响。
  ![sma-ema.png](sma-ema.png)
* Inflight：当前服务中正在进行的请求的数量。
* Pass & RT：pass 为成功请求的数量，rt 为平均响应时间。最近 5s 滑动窗口采样。
  ![moving-window.png](moving-window.png)

何时触发过载保护？
* 使用 CPU 的滑动均值(CPU > 800)作为启发阈值，一旦触发进入到过载保护阶段，
  算法为：(pass* rt) < inflight
* 限流效果生效后，CPU 会在临界值(800)附近抖动，
  如果不使用冷却时间，那么一个短时间的 CPU 下降就可能导致大量请求被放行，严重时会打满 CPU。
* 在冷却时间后，重新判断阈值(CPU > 800)，是否持续进入过载保护。

![cd-vs-no-cd.png](cd-vs-no-cd.png)

> 容器内 CPU 使用率计算有很多坑，小公司不要自己搞，建议直接抄 kratos。

> 过载保护是每一层服务都要做，不是只上层做。
> 过载保护是服务自己保护自己，如果下层不做，可能被上层放过的请求打死。

## 限流
限流是指在一段时间内，定义某个客户或应用可以接收或处理多少个请求的技术。
例如，通过限流，你可以过滤掉产生流量峰值的客户和微服务，
或者可以确保你的应用程序在自动扩展(Auto Scaling)完成前都不会出现过载的情况。
* 令牌桶、漏桶 针对单个节点，无法分布式限流。
* QPS 限流 (Go 语言不好实现（B站还没有实现）)
  * 不同的请求可能需要数量迥异的资源来处理。
  * 某种静态 QPS 限流不是特别准。
* 给每个用户设置限制
  * 全局过载发生时候，针对某些“异常”进行控制。
  * 一定程度的“超卖”配额。❓
* 按照优先级丢弃。
* 拒绝请求也是有成本的。  
  拒绝的响应也要消耗系统网络、进行上下文切换等。请求量级极大，过载保护也支撑不住。

### 分布式限流
分布式限流，是为了控制某个应用全局的流量，而非真对单个节点纬度。

使用 Redis 进行计数限流
* 单个大流量的接口，容易产生热点 key。
* pre-request 模式对性能有一定影响，高频的网络往返。
  每一个请求要调一次（或多次）redis，给 redis 增加负担的同时还影响请求 latency。

考虑：
从获取单个 quota 升级成批量 quota。 quota 表示速率，获取后使用令牌桶算法来限制。

![distributed-rate-limiting.png](distributed-rate-limiting.png)

* 每次心跳后，异步批量获取 quota，可以大大减少请求 redis 的频次，
  获取完以后本地消费，基于令牌桶拦截。
* 每次申请的配额需要手动设定静态值略欠灵活，比如每次要20，还是50。

Q: 如何基于单个节点按需申请，并且避免出现不公平的现象？  
A: 初次使用默认值，一旦有过去历史窗口的数据，可以基于历史窗口数据进行 quota 请求。

思考：
我们经常面临给一组用户划分稀有资源的问题，他们都享有等价的权利来获取资源，
但是其中一些用户实际上只需要比其他用户少的资源。

那么我们如何来分配资源呢？
一种在实际中广泛使用的分享技术称作“最大最小公平分享”(Max-Min Fairness)。

直观上，公平分享分配给每个用户想要的可以满足的最小需求，
然后将没有使用的资源均匀的分配给需要‘大资源’的用户。

![max-min-fairness.png](max-min-fairness.png)

最大最小公平分配算法的形式化定义如下：
* 资源按照需求递增的顺序进行分配。
* 不存在用户得到的资源超过自己的需求。
* 未得到满足的用户等价的分享资源。

> 进化版：多维度，DRF 算法

> TBD 分布式限流实现 + Doorman 直播回放 01:35:34 - 01:42:03

| 类型 | 优点 | 缺点 | 现有实现（开源）|
| --- | :--- | :--- | :--- |
| 单机限制 | 1. 实现简单<br/>2. 稳定可靠<br/>3. 性能高 | 1. 流量不均匀会引发误限制<br/> 2. 机器数变化时，配额需要手动调整，容易出错 | 各语言都有相应的实现 |
| 动态流控 | 1. 根据服务情况动态限流<br/> 2. 不用调整额度 | 1. 需要主动搜集请求的性能数据（CPU Load、成功率、耗时）<br/>2. 客户端主动善意限流<br/>3. 一般只限于接口调用，支持的范围小、应用场景狭窄 | BBR 限流<br/> 广义上各种连接池也属于这类 |
| 全局限流 | 1. 流量不均不会误触发限流<br/>2. 机器数变动时无需调整<br/>3. 应用场景丰富，接口、DB等任何资源都可以使用 | 1. 实现较复杂<br/> 2. 需要手动配置 | 无 |

> 分布式限流也还是解决不了极大流量请求无法承受的问题。

### 重要性分级
每个接口配置阈值，运营工作繁重，配置服务级别 quota 又太过粗糙。
可以根据不同重要性设定 quota，我们引入了重要性(criticality):
* 最重要 CRITICAL_PLUS，为最终的要求预留的类型。
  拒绝这些请求会造成非常严重的用户可见的问题。
* 重要 CRITICAL，生产任务发出的默认请求类型。
  拒绝这些请求也会造成用户可见的问题。但是可能没那么严重。
* 可丢弃的 SHEDDABLE_PLUS 这些流量可以容忍某种程度的不可用性。
  这是批量任务发出的请求的默认值。这些请求通常可以过几分钟、几小时后重试。
* 可丢弃的 SHEDDABLE 这些流量可能会经常遇到部分不可用情况，偶尔会完全不可用。

gRPC 系统之间，需要自动传递重要性信息。
如果后端接受到请求 A，在处理过程中发出了请求 B 和 C 给其他后端，
请求 B 和 C 会使用与 A 相同的重要性属性。
* 全局配额不足时，优先拒绝低优先级的。
* 全局配额，可以按照重要性分别设置。
* 过载保护时，低优先级的请求先被拒绝。

### 熔断
断路器(Circuit Breakers): 为了限制操作的持续时间，我们可以使用超时，
超时可以防止挂起操作并保证系统可以响应。
因为我们处于高度动态的环境中，几乎不可能确定在每种情况下都能正常工作的准确的时间限制。
断路器以现实世界的电子元件命名，因为它们的行为是都是相同的。
断路器在分布式系统中非常有用，因为重复的故障可能会导致雪球效应，并使整个系统崩溃。

* 服务依赖的资源出现大量错误。
* 某个用户超过资源配额时，后端任务会快速拒绝请求，返回“配额不足”的错误，
  但是拒绝回复仍然会消耗一定资源。有可能后端忙着不停发送拒绝请求，导致过载。
  > 断路器要在客户端实现

![hystrix-state.png](hystrix-state.png)
Hystrix 在断路器打开的情况下会完全阻断请求，“一刀切”的处理方式过于暴力，
大量请求丢失，会导致大面积服务不可用。

Google SRE 做法：  
max(0, (requests - K*accepts) / (requests + 1))  
其中 requests 是总请求数，accepts 是成功的请求，K 是一个可配置的常量（先假设取值为2）。
当开始出现请求失败时，accepts 基本保持在一个稳定值上。
公式的结果最终是一个比值结果，表示需要丢弃的请求占比。
K 的值表示限流的激进程度，值越小，表示越激进，值越大表示越保守。
如 K 为1时，一开始出现错误就开始丢弃流量，K 为10时，到只有一成成功时才开始丢弃流量。

![src-circuit-breaker-effect.png](sre-circuit-breaker-effect.png)
蓝色线表示请求流量，绿色线表示丢弃流量。

![sre-circuit-breaker-implementation.png](sre-circuit-breaker-implementation.png)

### 客户端流控
positive feedback: 用户总是积极重试，访问一个不可达的服务。
* 客户端需要限制请求频次，retry backoff 做一定的请求退让。
  ![client-backoff.png](client-backoff.png)
* 退让时长可以通过接口级别的error_details，挂载到每个 API 返回的响应里。
  ![retry-policy-via-response.png](retry-policy-via-response.png)

![jitter-backoff-implementation.png](jitter-backoff-implementation.png)

### Gutter 🤔
> 有点像水库大坝的溢洪道的思路。

基于熔断的 gutter kafka ，
用于接管自动修复系统运行过程中的负载，
这样只需要付出10%的资源就能解决部分系统可用性问题。

我们经常使用 failover 的思路，
但是完整的 failover 需要翻倍的机器资源，平常不接受流量时，资源浪费。
高负载情况下接管流量又不一定完整能接住。
所以这里核心利用熔断的思路，是把抛弃的流量转移到 gutter 集群，
如果 gutter 也接受不住的流量，重新回抛到主集群，最大力度来接受。

![gutter.png](gutter.png)

## 案例
### Case 1
二层缓存穿透、大量回源导致的核心服务故障。

参考第五周评论系统中缓存 miss 回源方案：kafka + single-flight

### Case 2
异常客户端引起的服务故障（query of death）
* 请求放大
* 资源数放大

控制异常请求

### Case 3
用户重试导致的大面积故障。

客户端流控 + 熔断 + 限流 + 过载保护共同应对。

## 降级
## 重试
## 负载均衡
## 最佳实践
